{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8sFElPqq8PPp"
   },
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "x6GhEWW18aCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-30 21:13:10--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228145 (223K) [text/plain]\n",
      "Saving to: ‘names.txt.2’\n",
      "\n",
      "names.txt.2         100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-06-30 21:13:10 (5.09 MB/s) - ‘names.txt.2’ saved [228145/228145]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the names.txt file from github\n",
    "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3471, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = h @ W2 + b2\n",
    "# dlogits/dh = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c (3x4) = a(3x2) @ b(2x4)\n",
    "\n",
    "# c (3x4) @ b^-1(4x2) = a(3x2)\n",
    "\n",
    "# a11 a12\n",
    "# a21 a22\n",
    "# a31 a32\n",
    "\n",
    "# b11 b12 b13 b14\n",
    "# b21 b22 b23 b24\n",
    "\n",
    "# c11 = a11 * b11 + a12 * b21 | a11 * b12 + a12 * b22 | ...\n",
    "# c21 = a21 * b11 + a22 * b21 | a21 * b12 + a22 * b22 | ...\n",
    "# c31 = a31 * b11 + a32 * b21 | a31 * b12 + a32 * b22 | ...\n",
    "\n",
    "# b11 | b12 | b13 | b14\n",
    "#  0. |  0. |. 0. |. 0\n",
    "#  0  |  0. |. 0  |  0\n",
    "\n",
    "# dc11 = b11/da11 + b21/da12\n",
    "\n",
    "# dL/da_1,1 = dL/dd_1,1*dd_1,1/da_1,1 + dL/dd_1,2*dd_1,2/da_1,1\n",
    "#                       b_1,1                     b_1,2\n",
    "\n",
    "\n",
    "# dL/db = dL/dc * dc/db\n",
    "# dL/db = dL/dc * 1\n",
    "\n",
    "# dL/dbngain = dL/dhpre * dhpre/dbngain\n",
    "#                         bnraw\n",
    "\n",
    "# dL/dbndiff2 = dL/dbnvar * dbnvar/dbndiff2\n",
    "                            \n",
    "\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14fab4f40>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dlogits_maxmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3935e-04, -1.5308e-03,  3.5536e-04,  7.8326e-04, -1.3437e-03,\n",
       "         -3.0134e-03,  4.7998e-03,  8.3405e-05,  4.2805e-05,  2.1478e-03],\n",
       "        [-2.3346e-03,  1.1557e-03,  1.8261e-03,  3.7018e-03, -6.5865e-04,\n",
       "         -2.2421e-04,  1.0325e-03,  2.9088e-03,  3.6548e-03, -4.5196e-03],\n",
       "        [ 3.7712e-03,  4.4422e-03,  1.6773e-04,  5.1375e-03, -5.0867e-03,\n",
       "          1.1939e-03,  4.0165e-03,  1.6160e-03,  4.8546e-03, -1.7404e-03]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.3935e-04, -1.5308e-03,  3.5536e-04,  7.8326e-04, -1.3437e-03,\n",
       "          -3.0134e-03,  4.7998e-03,  8.3405e-05,  4.2805e-05,  2.1478e-03],\n",
       "         [-2.3346e-03,  1.1557e-03,  1.8261e-03,  3.7018e-03, -6.5865e-04,\n",
       "          -2.2421e-04,  1.0325e-03,  2.9088e-03,  3.6548e-03, -4.5196e-03],\n",
       "         [ 3.7712e-03,  4.4422e-03,  1.6773e-04,  5.1375e-03, -5.0867e-03,\n",
       "           1.1939e-03,  4.0165e-03,  1.6160e-03,  4.8546e-03, -1.7404e-03]],\n",
       "\n",
       "        [[-1.4828e-03, -4.0757e-03, -6.1243e-04,  6.7813e-03,  1.7882e-03,\n",
       "          -6.1342e-03,  3.6754e-03, -1.8913e-03, -2.3356e-03,  1.4082e-03],\n",
       "         [-1.3780e-03, -2.5266e-03, -4.9496e-04, -1.2087e-03,  2.6468e-03,\n",
       "          -4.4593e-03,  6.3397e-04, -2.7163e-03,  1.6697e-03,  7.9554e-04],\n",
       "         [ 1.9671e-03, -2.1791e-03,  5.7139e-04,  2.8990e-04, -6.1014e-03,\n",
       "           5.6128e-04, -2.6025e-03, -3.0784e-04, -2.1888e-03,  7.5408e-03]],\n",
       "\n",
       "        [[-7.3335e-04, -4.1496e-03,  5.1946e-03,  5.4497e-03,  2.2966e-03,\n",
       "           5.8039e-03,  4.8058e-03, -2.9124e-03, -1.0258e-03, -2.9130e-03],\n",
       "         [-4.5733e-03,  4.5951e-03,  1.7588e-03, -7.3753e-03, -3.2078e-03,\n",
       "          -2.9422e-03,  3.0030e-03, -9.0385e-04,  3.6510e-03,  3.3910e-03],\n",
       "         [-4.8659e-03,  7.4039e-03,  1.0266e-03,  1.0881e-03, -4.5669e-03,\n",
       "          -7.2924e-04, -4.4086e-03,  5.1176e-03, -1.9293e-03, -1.1454e-03]],\n",
       "\n",
       "        [[ 2.4038e-03,  2.7227e-03,  6.5750e-03,  7.6202e-03, -4.9284e-03,\n",
       "           3.6033e-03,  7.7430e-04, -8.1016e-04, -6.0052e-03,  1.3241e-03],\n",
       "         [-3.1177e-03,  6.5438e-03,  3.6685e-03,  1.5374e-03, -4.8750e-03,\n",
       "           1.0489e-02,  5.4006e-03,  1.6716e-03, -8.0489e-03, -1.0754e-03],\n",
       "         [-9.9460e-03, -5.6559e-03,  5.4161e-03, -3.0212e-03,  1.1454e-02,\n",
       "          -1.4055e-03,  5.1920e-03, -7.6910e-03, -2.9903e-03, -1.4992e-03]],\n",
       "\n",
       "        [[ 2.3826e-03, -3.0136e-03, -2.8838e-03,  2.2909e-03, -3.7603e-03,\n",
       "           1.6562e-03, -3.5401e-03, -5.4479e-03, -1.3890e-03,  1.0461e-04],\n",
       "         [-2.7099e-04,  1.1344e-03, -3.1273e-05, -3.2181e-03,  1.4752e-04,\n",
       "           2.6393e-03,  2.0313e-03,  8.0153e-04, -4.4222e-03, -7.7511e-04],\n",
       "         [ 1.8868e-03,  2.6410e-04,  2.9224e-03, -4.1745e-03,  5.7769e-03,\n",
       "           2.2424e-03, -3.6717e-03,  5.6925e-03, -1.8581e-03, -5.2175e-03]],\n",
       "\n",
       "        [[ 6.0818e-03, -2.9458e-04,  6.0477e-03,  7.0433e-03, -4.5917e-03,\n",
       "          -6.7675e-04,  2.5369e-03, -5.8596e-04,  4.2194e-04, -4.2035e-03],\n",
       "         [ 1.7233e-03, -7.9695e-04,  1.5764e-03,  4.1926e-03, -9.7020e-03,\n",
       "           4.9102e-03, -3.7242e-03,  1.6752e-03,  3.8662e-03,  7.2429e-04],\n",
       "         [ 3.4669e-03,  5.4840e-04,  1.4025e-03,  6.7737e-03,  4.6517e-03,\n",
       "          -7.4869e-03,  2.8878e-03,  2.8396e-04,  4.8467e-04,  6.7039e-04]],\n",
       "\n",
       "        [[-4.8636e-03,  8.7248e-04, -1.8257e-03, -6.1608e-03,  2.4244e-03,\n",
       "           1.3324e-03,  2.8961e-03,  2.9346e-03,  2.1815e-03,  2.8384e-05],\n",
       "         [-1.1031e-03,  3.1926e-03, -1.0217e-03, -3.4542e-03,  1.1969e-03,\n",
       "          -3.5593e-05,  2.7911e-04, -6.9271e-04,  7.6023e-04, -2.5910e-04],\n",
       "         [-1.2043e-03, -1.0426e-03, -2.0106e-03, -3.4866e-03,  9.9320e-04,\n",
       "           5.0439e-04,  2.5543e-03, -7.8832e-03, -8.1543e-04, -5.4375e-03]],\n",
       "\n",
       "        [[-5.8939e-03, -4.4338e-03,  5.1834e-03,  4.1521e-03,  2.7915e-03,\n",
       "          -4.1486e-03, -6.5301e-04, -4.9275e-03, -3.4749e-03,  2.6847e-03],\n",
       "         [-2.3969e-04, -4.7635e-03, -2.9331e-03, -3.6371e-03,  1.4957e-03,\n",
       "          -4.6615e-03, -2.3482e-03, -4.3974e-03,  2.1445e-03,  3.9247e-03],\n",
       "         [ 1.4688e-03, -1.9303e-03,  3.2283e-03, -2.2608e-03,  4.0844e-03,\n",
       "           1.3532e-05, -7.8368e-05, -6.7897e-04, -3.8883e-03,  4.2407e-03]],\n",
       "\n",
       "        [[-2.7275e-03,  3.6522e-03, -2.4032e-05, -2.2960e-03, -1.3409e-03,\n",
       "          -7.4180e-04, -4.1050e-03,  6.2755e-03, -3.2124e-04, -1.2612e-03],\n",
       "         [ 5.2876e-03, -6.6132e-03, -5.0027e-03,  3.7201e-03,  6.5543e-03,\n",
       "          -1.0801e-03, -4.1790e-03, -4.9416e-03,  1.2994e-03, -2.3715e-03],\n",
       "         [ 7.4392e-03,  1.1180e-03, -1.1783e-03, -1.5709e-04, -5.8066e-03,\n",
       "           3.0005e-04, -5.5916e-04, -3.4939e-03,  2.2602e-03,  4.7776e-03]],\n",
       "\n",
       "        [[ 2.1146e-03, -3.9120e-03, -2.6305e-03, -1.0318e-02,  2.9283e-03,\n",
       "          -7.9082e-03, -2.6850e-05,  5.9826e-04,  1.5948e-04, -7.6365e-04],\n",
       "         [-1.5334e-03, -3.8139e-03, -1.1664e-03,  2.6165e-03, -4.8656e-03,\n",
       "           7.2299e-04,  1.6315e-04,  8.5175e-04, -4.7708e-03,  3.2423e-03],\n",
       "         [ 3.7073e-04, -4.4287e-03,  2.1228e-03,  1.6149e-03,  4.1694e-04,\n",
       "          -5.1792e-04, -6.6007e-04,  4.0236e-03,  1.2989e-03,  4.5629e-03]],\n",
       "\n",
       "        [[ 2.0740e-04, -1.3106e-03,  1.4725e-03,  3.0866e-04, -6.4501e-03,\n",
       "           2.9640e-03,  3.0901e-03, -4.1056e-03, -1.8373e-03,  6.3532e-04],\n",
       "         [-3.7773e-03, -3.9360e-03, -9.0463e-04, -1.3392e-03,  5.1608e-04,\n",
       "          -9.0477e-04, -6.4931e-03,  1.8288e-03, -2.6320e-03, -6.9432e-04],\n",
       "         [ 5.0720e-04,  4.0745e-03, -4.7769e-03, -4.9466e-04, -2.3037e-03,\n",
       "          -8.8325e-05,  2.8529e-03, -2.6761e-03,  3.3720e-03, -3.9354e-03]],\n",
       "\n",
       "        [[ 9.4178e-04,  2.1308e-03, -3.1746e-03, -2.1182e-03, -3.4343e-03,\n",
       "           4.2038e-03, -1.8563e-03,  1.6184e-03,  2.8949e-03,  3.4724e-04],\n",
       "         [ 5.2512e-04, -8.5393e-04,  6.5630e-04, -2.4397e-03, -5.8309e-04,\n",
       "          -7.3585e-04, -9.1945e-04,  1.6492e-03,  4.9559e-04, -2.5134e-03],\n",
       "         [ 2.5996e-03,  1.5444e-03, -4.9915e-03,  7.9549e-04, -3.3667e-03,\n",
       "          -3.3424e-03, -4.7152e-03,  8.2408e-03, -4.5463e-03,  7.7708e-04]],\n",
       "\n",
       "        [[-1.6265e-03, -1.4581e-03, -5.1082e-03, -5.2119e-03,  3.4569e-03,\n",
       "           1.5628e-03,  1.5214e-03,  1.7621e-03, -1.9059e-03, -1.9900e-03],\n",
       "         [-2.3160e-03, -2.2571e-03, -1.6234e-03,  1.9097e-04,  1.2875e-03,\n",
       "          -1.3390e-03,  2.5646e-03,  1.8662e-03,  7.8271e-03, -9.1173e-03],\n",
       "         [ 6.2013e-03,  4.1209e-03, -4.1273e-03,  6.1088e-03, -8.2510e-03,\n",
       "           3.9121e-03,  2.0120e-03,  3.5484e-04,  5.4855e-03,  2.4905e-03]],\n",
       "\n",
       "        [[-1.9579e-03, -2.7642e-03,  2.4189e-03,  8.3019e-03,  2.8692e-03,\n",
       "          -4.2794e-03,  5.3704e-03, -2.9561e-03, -2.6985e-03,  2.1206e-03],\n",
       "         [-3.1811e-03, -1.0778e-03, -9.0023e-04,  1.8316e-03,  5.0138e-04,\n",
       "          -3.3028e-03, -9.3517e-04, -2.0447e-03,  1.7688e-03,  2.4612e-03],\n",
       "         [ 1.2282e-03, -5.3816e-03,  3.8414e-03, -9.1935e-04, -1.9834e-03,\n",
       "          -2.5514e-03, -9.3662e-04, -4.1478e-03, -2.5446e-03,  6.3758e-03]],\n",
       "\n",
       "        [[-9.0623e-03,  1.1530e-03,  4.5671e-03,  5.3440e-03,  3.5685e-03,\n",
       "          -2.2979e-03,  9.1146e-04, -2.6433e-03, -2.6303e-03,  1.2438e-03],\n",
       "         [ 4.2970e-03, -1.6475e-03, -1.9893e-03, -1.8656e-03, -6.8414e-04,\n",
       "          -2.8070e-03, -6.8823e-05,  9.3927e-04,  5.0707e-03, -1.1643e-03],\n",
       "         [-2.2918e-03, -7.2080e-03, -1.2859e-03,  1.3908e-03,  6.1942e-03,\n",
       "           4.5581e-04,  3.0541e-03,  3.3253e-03,  2.8073e-03, -1.4563e-03]],\n",
       "\n",
       "        [[ 7.3541e-03,  1.3127e-03,  1.2775e-03,  1.3716e-03,  1.2625e-03,\n",
       "           4.4118e-03, -4.3940e-03, -6.8324e-03,  3.4427e-03,  3.6144e-03],\n",
       "         [ 2.5919e-03,  6.4926e-03,  8.9498e-04,  3.0765e-03, -1.5839e-03,\n",
       "           2.3340e-03,  2.4457e-03,  4.2995e-03, -3.1707e-03,  3.3226e-03],\n",
       "         [-4.1145e-03, -2.7915e-03,  1.8790e-03, -4.2036e-03,  7.6537e-03,\n",
       "          -1.1954e-03, -3.9935e-03,  6.0324e-03, -1.4881e-03,  1.2462e-04]],\n",
       "\n",
       "        [[-8.6800e-04,  6.8781e-04,  6.1898e-03,  1.9360e-03,  2.1810e-03,\n",
       "           1.5346e-03, -5.4184e-04,  1.2233e-03, -4.3928e-04, -1.5872e-03],\n",
       "         [ 2.8366e-03,  4.4062e-03, -4.9131e-04,  2.1801e-03, -5.1983e-03,\n",
       "          -1.4084e-04,  7.4673e-03,  2.4466e-03,  9.7200e-04,  5.6655e-03],\n",
       "         [-2.1528e-03, -9.8707e-04,  1.7038e-03,  6.3268e-04,  7.7237e-03,\n",
       "          -7.3239e-05, -2.0447e-03,  7.4929e-03,  4.3644e-03, -2.1606e-03]],\n",
       "\n",
       "        [[ 4.9841e-03, -2.6455e-03,  3.9202e-03, -5.4965e-03,  1.4279e-03,\n",
       "           1.2691e-03, -3.5589e-03, -7.9235e-03, -1.9357e-03,  6.2051e-04],\n",
       "         [ 6.1636e-04,  3.3830e-03,  7.1542e-04,  5.5670e-04,  1.8147e-03,\n",
       "          -1.2327e-03,  8.7249e-04,  2.9893e-03, -2.8687e-03,  3.5938e-03],\n",
       "         [ 1.7602e-03,  2.4431e-03,  6.3713e-03, -1.7373e-03,  6.1027e-03,\n",
       "           2.5472e-03, -6.2715e-03, -1.6417e-03, -5.7903e-04, -1.9357e-03]],\n",
       "\n",
       "        [[-1.1202e-03, -3.6203e-03, -7.1800e-03,  6.7358e-03, -5.9389e-03,\n",
       "           8.2553e-04,  1.0357e-03, -1.2251e-03, -3.3840e-03, -2.6368e-03],\n",
       "         [-4.1171e-03, -2.9227e-03,  6.2071e-04, -2.4155e-03,  2.3374e-03,\n",
       "           2.2665e-03, -6.4193e-03, -1.7277e-03,  1.6505e-03,  1.7031e-03],\n",
       "         [-2.9264e-03,  5.1207e-03, -7.9029e-03,  2.5974e-03, -1.1463e-02,\n",
       "           1.7641e-03, -8.8071e-04,  4.5292e-03, -9.2799e-04, -2.4200e-03]],\n",
       "\n",
       "        [[-3.8800e-03,  7.5544e-04, -4.5707e-03, -1.5492e-03,  1.3302e-04,\n",
       "           5.0070e-04,  6.9635e-04,  3.3835e-03,  3.9274e-04, -3.2809e-04],\n",
       "         [-1.9410e-03,  1.9749e-03, -9.7621e-04, -5.2975e-03,  2.9147e-03,\n",
       "          -1.3126e-03,  9.8442e-04, -4.5393e-03,  2.5701e-03,  2.4793e-04],\n",
       "         [-1.1904e-03, -1.0116e-03, -4.9731e-03, -8.6512e-05, -6.1353e-04,\n",
       "           5.0473e-04,  3.2344e-03, -6.6206e-03, -1.7246e-03, -3.2982e-03]],\n",
       "\n",
       "        [[-5.4530e-03,  5.3405e-03, -3.7195e-04, -9.2002e-04,  2.4097e-03,\n",
       "          -7.6670e-05,  1.1531e-03,  8.6315e-03, -3.8763e-04, -3.4189e-03],\n",
       "         [ 5.2061e-04, -1.0784e-04, -4.0531e-03, -7.4410e-04,  6.3520e-03,\n",
       "          -1.6767e-03, -2.3299e-03, -2.2701e-05, -2.4460e-03, -5.4698e-03],\n",
       "         [ 2.1268e-03,  1.8920e-03, -7.0963e-04, -2.5201e-03, -2.4834e-03,\n",
       "           1.3835e-04,  3.7592e-04, -3.4588e-05,  2.1778e-03, -3.9219e-04]],\n",
       "\n",
       "        [[ 6.0904e-03, -1.5696e-03, -3.3001e-03,  1.2234e-03, -2.5568e-04,\n",
       "           3.1347e-03, -3.7168e-03, -5.6369e-03, -1.8918e-03,  4.3242e-03],\n",
       "         [-3.8294e-04,  1.5921e-03,  1.8665e-03,  2.2746e-03,  1.2740e-03,\n",
       "          -2.5559e-04,  1.4752e-03,  2.9155e-03, -2.1312e-03,  3.0783e-03],\n",
       "         [-2.5277e-03,  4.1227e-04,  3.7001e-03, -4.2363e-03,  3.1127e-03,\n",
       "           2.8758e-03, -6.6427e-03,  2.1926e-03, -4.3443e-04, -1.9600e-03]],\n",
       "\n",
       "        [[ 2.7660e-03, -3.7642e-03, -3.0784e-03, -8.9294e-03,  3.8416e-04,\n",
       "          -7.1581e-03, -1.6111e-03,  7.3134e-04, -1.7635e-03, -2.4379e-03],\n",
       "         [-1.0779e-03, -2.3770e-03, -3.1371e-03,  4.6668e-03, -9.2766e-04,\n",
       "           1.0267e-03,  1.1664e-04,  1.3056e-03, -3.7989e-03,  4.0963e-03],\n",
       "         [-5.4851e-03, -4.5247e-03,  4.2378e-03,  1.0601e-03,  4.2656e-04,\n",
       "          -1.6265e-03, -2.9649e-03,  2.6899e-03,  1.1660e-03,  3.8893e-03]],\n",
       "\n",
       "        [[ 1.3992e-03, -7.0353e-04, -1.5319e-03, -6.7099e-03,  8.1300e-04,\n",
       "          -1.1159e-02,  1.2423e-04,  3.1379e-03, -4.1798e-04, -2.6587e-04],\n",
       "         [-1.1782e-03, -4.2742e-03, -6.0585e-04,  4.1589e-03, -8.5928e-04,\n",
       "          -2.1608e-03, -8.0758e-04,  1.6502e-03, -2.6503e-03,  3.6366e-03],\n",
       "         [-1.4262e-03, -2.1866e-03,  9.8664e-04,  7.2323e-04, -8.0479e-04,\n",
       "           2.9667e-03,  2.8417e-04,  4.7477e-03,  2.4558e-03,  4.3933e-03]],\n",
       "\n",
       "        [[ 4.6371e-03, -2.0618e-04, -4.2181e-04,  4.4554e-03, -3.5701e-03,\n",
       "           3.8878e-03,  1.8555e-03,  4.9966e-03, -4.9390e-04, -4.0990e-04],\n",
       "         [ 5.5890e-04,  6.6286e-03,  1.8994e-03,  2.6526e-03, -5.0628e-03,\n",
       "           7.9295e-03, -2.4274e-03,  5.5631e-03, -7.4698e-03, -3.0132e-03],\n",
       "         [-2.2785e-03, -5.3087e-04, -2.8326e-03,  3.7883e-03,  1.0918e-03,\n",
       "          -6.4205e-03,  1.1881e-03, -8.4487e-04, -5.7407e-03, -4.1202e-03]],\n",
       "\n",
       "        [[ 4.2377e-04,  5.6550e-03,  6.3336e-03,  8.0701e-04,  1.1446e-03,\n",
       "           1.2132e-03,  4.9342e-05, -2.0260e-03,  5.4895e-03,  1.8922e-05],\n",
       "         [ 2.3013e-03,  2.5498e-03,  5.7829e-03, -7.7825e-05,  6.2946e-04,\n",
       "           1.3104e-03,  4.5002e-03, -2.0555e-03,  1.5076e-03,  2.8642e-03],\n",
       "         [-5.7144e-03,  3.6796e-03,  1.2188e-03, -4.8183e-04,  9.1110e-04,\n",
       "          -8.1753e-04,  6.5653e-03, -6.2764e-03,  1.1730e-04,  1.9250e-03]],\n",
       "\n",
       "        [[-5.6742e-03, -2.2034e-03, -6.2893e-03, -3.4242e-03, -5.4889e-04,\n",
       "           2.5032e-03,  1.1894e-03,  4.4817e-03, -1.5959e-04,  5.1577e-04],\n",
       "         [-2.6744e-03,  3.3870e-03, -9.0011e-04, -6.8472e-03, -1.0437e-03,\n",
       "          -1.6465e-03, -3.2688e-03, -2.1818e-03,  1.6453e-03, -2.6760e-03],\n",
       "         [ 1.4104e-03, -1.4258e-03, -3.1383e-03, -6.0239e-04, -2.6197e-03,\n",
       "          -2.6016e-05,  2.7577e-03, -6.8613e-03, -1.8568e-03, -6.2041e-03]],\n",
       "\n",
       "        [[ 5.6251e-03,  6.9112e-03, -6.3536e-03, -6.1578e-03, -6.9121e-03,\n",
       "           6.1530e-03, -4.1312e-03,  4.3919e-03,  2.9452e-03,  1.4188e-04],\n",
       "         [ 1.6976e-03, -3.3634e-03,  4.1203e-03,  1.7845e-03,  3.4150e-03,\n",
       "          -9.1820e-04, -7.7450e-03, -1.2530e-03, -6.5913e-03, -8.7238e-03],\n",
       "         [ 3.9348e-03,  5.0553e-03, -4.8405e-03,  1.4516e-03, -7.8446e-03,\n",
       "           9.1225e-04,  1.2019e-03,  6.2956e-03, -4.1543e-04,  1.1796e-03]],\n",
       "\n",
       "        [[-4.5554e-04,  8.2387e-03, -9.9000e-04,  3.9474e-05,  2.8942e-03,\n",
       "           6.4254e-04, -3.5754e-03,  5.8860e-03,  5.5812e-03, -1.0252e-03],\n",
       "         [ 7.5269e-03, -8.1415e-03, -6.0191e-04,  3.0333e-03,  4.4585e-03,\n",
       "          -2.2859e-03, -4.3675e-04, -5.6434e-03,  2.4532e-03, -2.2652e-03],\n",
       "         [ 1.0426e-02, -1.2125e-03, -2.5967e-04, -3.0863e-03, -7.0131e-03,\n",
       "           2.2984e-03, -4.4876e-04, -1.3268e-03,  9.1523e-04,  3.8667e-03]],\n",
       "\n",
       "        [[-5.7606e-03,  6.2895e-04, -4.5762e-03, -4.3092e-03,  5.5411e-03,\n",
       "           1.5661e-03,  1.6692e-03,  5.1448e-03,  3.0483e-03,  2.7968e-03],\n",
       "         [-6.9068e-05,  1.1413e-03, -1.1709e-03, -2.8122e-03, -4.8305e-04,\n",
       "          -1.5627e-03,  1.4107e-03, -2.6357e-03,  3.5303e-03, -1.7316e-03],\n",
       "         [ 7.2568e-04, -3.6934e-03, -3.0731e-03, -1.8538e-03,  3.6612e-03,\n",
       "           2.0418e-03,  2.8965e-03, -1.0347e-02,  1.9868e-04, -3.1054e-03]],\n",
       "\n",
       "        [[ 7.7900e-04, -3.6740e-03,  6.2475e-04,  5.9914e-04,  1.2930e-03,\n",
       "           1.9271e-03, -5.9221e-03, -5.7278e-03,  1.5386e-04,  1.3787e-03],\n",
       "         [ 1.1530e-03,  2.7710e-03, -1.4682e-03,  6.4078e-04, -1.3125e-03,\n",
       "           1.7165e-03,  1.0583e-03,  2.7066e-03, -3.0239e-04, -1.5763e-03],\n",
       "         [ 8.0556e-04, -9.0732e-05,  6.3057e-03, -1.7274e-03,  5.3677e-03,\n",
       "           9.7219e-04, -4.3701e-03,  1.5965e-03,  3.2875e-04, -5.5206e-03]],\n",
       "\n",
       "        [[ 2.8293e-03,  5.2683e-03,  4.7628e-03, -1.6423e-03,  1.4674e-03,\n",
       "          -3.1017e-03, -5.2206e-04,  3.7125e-04,  7.7430e-03, -2.2147e-03],\n",
       "         [ 3.6297e-03, -1.4750e-03,  4.0860e-03, -8.3538e-05,  3.5056e-03,\n",
       "           3.3941e-04,  6.6635e-03, -2.3131e-03,  4.7660e-03,  5.1985e-03],\n",
       "         [-5.9724e-03,  4.1618e-03, -1.0022e-03,  1.5972e-03,  6.8601e-04,\n",
       "           7.5773e-05,  4.1751e-03, -3.3991e-03,  1.6412e-03,  4.7342e-03]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0029, -0.0090,  0.0250, -0.0033,  0.0291, -0.0112, -0.0057, -0.0062,\n",
       "        -0.0070, -0.0014], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "\n",
    "dlogprobs = torch.zeros((32,27))\n",
    "dlogprobs[range(n), Yb] = -1/n\n",
    "\n",
    "dprobs = 1/probs * dlogprobs\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs + dcounts_sum\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdims=True)\n",
    "\n",
    "dlogits_maxmat = torch.zeros((32,27))\n",
    "dlogits_maxmat[range(n), logits.max(1).indices] = 1\n",
    "dlogits = dnorm_logits.clone() + dlogits_maxmat * dlogit_maxes\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "dW2 = h.T @ dlogits\n",
    "\n",
    "db2 = dlogits.sum(0, keepdims=True)\n",
    "\n",
    "dhpreact = (1-h**2) * dh\n",
    "\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdims=True)\n",
    "\n",
    "dbnraw = torch.ones((32,64)) * bngain * dhpreact\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)\n",
    "\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "\n",
    "dbndiff2 = torch.ones((32,64)) / (n-1) * dbnvar\n",
    "\n",
    "dbndiff = 2 * bndiff * dbndiff2 + bnvar_inv * dbnraw\n",
    "\n",
    "dbnmeani = -dbndiff.sum(0, keepdims=True)\n",
    "\n",
    "dhprebn = dbndiff + torch.ones((32,64)) / n * dbnmeani\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "db1 = dhprebn.sum(0, keepdims=True)\n",
    "\n",
    "demb = dembcat.view((32,3,10))\n",
    "\n",
    "dC = torch.zeros((27,10))\n",
    "for j in range(Xb.shape[0]):\n",
    "  for k in range(Xb.shape[1]):\n",
    "    dC[Xb[j,k]] += demb[j,k]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "ebLtYji_8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3471269607543945 diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "-gCXbB4C8PPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(batch_size), Yb] -= 1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "hd-MkhB68PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "POdeZSKT8PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain * bnvar_inv / n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "wPy8DhqB8PPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.8405\n",
      "  10000/ 200000: 2.1963\n",
      "  20000/ 200000: 2.3730\n",
      "  30000/ 200000: 2.4652\n",
      "  40000/ 200000: 1.9839\n",
      "  50000/ 200000: 2.3454\n",
      "  60000/ 200000: 2.4209\n",
      "  70000/ 200000: 2.0129\n",
      "  80000/ 200000: 2.3625\n",
      "  90000/ 200000: 2.1271\n",
      " 100000/ 200000: 1.9724\n",
      " 110000/ 200000: 2.3388\n",
      " 120000/ 200000: 1.9897\n",
      " 130000/ 200000: 2.4415\n",
      " 140000/ 200000: 2.2716\n",
      " 150000/ 200000: 2.2162\n",
      " 160000/ 200000: 1.9143\n",
      " 170000/ 200000: 1.8019\n",
      " 180000/ 200000: 1.9636\n",
      " 190000/ 200000: 1.8715\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "  #   loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    # YOUR CODE HERE :)\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(batch_size), Yb] -= 1\n",
    "    dlogits /= n\n",
    "\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "\n",
    "    dhpreact = (1-h**2) * dh\n",
    "\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv / n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for j in range(Xb.shape[0]):\n",
    "      for k in range(Xb.shape[1]):\n",
    "        dC[Xb[j,k]] += demb[j,k]\n",
    "\n",
    "  #   dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  #   print(dC.shape)\n",
    "  #   print(dbnbias.shape)\n",
    "  #   print(p.data.shape)\n",
    "  #   print((p.data + (-lr * grad)).shape)\n",
    "  #   print(grads[-1].shape)\n",
    "  #   print(parameters[-1].shape)\n",
    "  #   print(dC.shape)\n",
    "    for p, grad in zip(parameters, grads):\n",
    "  #     p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "  #     print(p.data.shape)\n",
    "  #     print(grad.shape)\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "#     if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "#       break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "ZEpI0hMW8PPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KImLWNoh8PP0"
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aFnP_Zc8PP0"
   },
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esWqmhyj8PP1"
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHeQNv3s8PP1"
   },
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
